{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6df0db30",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-12-29T06:39:31.133158Z",
     "iopub.status.busy": "2021-12-29T06:39:31.131898Z",
     "iopub.status.idle": "2021-12-29T06:39:31.148399Z",
     "shell.execute_reply": "2021-12-29T06:39:31.149101Z",
     "shell.execute_reply.started": "2021-12-29T05:52:17.824438Z"
    },
    "papermill": {
     "duration": 0.02835,
     "end_time": "2021-12-29T06:39:31.149486",
     "exception": false,
     "start_time": "2021-12-29T06:39:31.121136",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/nlp-getting-started/sample_submission.csv\n",
      "/kaggle/input/nlp-getting-started/train.csv\n",
      "/kaggle/input/nlp-getting-started/test.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bd8232",
   "metadata": {
    "papermill": {
     "duration": 0.005435,
     "end_time": "2021-12-29T06:39:31.161658",
     "exception": false,
     "start_time": "2021-12-29T06:39:31.156223",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0942e835",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-29T06:39:31.185467Z",
     "iopub.status.busy": "2021-12-29T06:39:31.184622Z",
     "iopub.status.idle": "2021-12-29T06:40:09.140044Z",
     "shell.execute_reply": "2021-12-29T06:40:09.140585Z",
     "shell.execute_reply.started": "2021-12-29T06:38:10.635476Z"
    },
    "papermill": {
     "duration": 37.973519,
     "end_time": "2021-12-29T06:40:09.140814",
     "exception": false,
     "start_time": "2021-12-29T06:39:31.167295",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8008668657458179\n",
      "         id keyword location  \\\n",
      "0         1     NaN      NaN   \n",
      "1         4     NaN      NaN   \n",
      "2         5     NaN      NaN   \n",
      "3         6     NaN      NaN   \n",
      "4         7     NaN      NaN   \n",
      "...     ...     ...      ...   \n",
      "7608  10869     NaN      NaN   \n",
      "7609  10870     NaN      NaN   \n",
      "7610  10871     NaN      NaN   \n",
      "7611  10872     NaN      NaN   \n",
      "7612  10873     NaN      NaN   \n",
      "\n",
      "                                                   text  target  \n",
      "0     Our Deeds are the Reason of this #earthquake M...       1  \n",
      "1                Forest fire near La Ronge Sask. Canada       1  \n",
      "2     All residents asked to 'shelter in place' are ...       1  \n",
      "3     13,000 people receive #wildfires evacuation or...       1  \n",
      "4     Just got sent this photo from Ruby #Alaska as ...       1  \n",
      "...                                                 ...     ...  \n",
      "7608  Two giant cranes holding a bridge collapse int...       1  \n",
      "7609  @aria_ahrary @TheTawniest The out of control w...       1  \n",
      "7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1  \n",
      "7611  Police investigating after an e-bike collided ...       1  \n",
      "7612  The Latest: More Homes Razed by Northern Calif...       1  \n",
      "\n",
      "[7613 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import keras.models\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "from keras.applications.densenet import layers\n",
    "from keras.regularizers import l2\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from sklearn.naive_bayes import MultinomialNB,CategoricalNB\n",
    "from keras_preprocessing.text import Tokenizer\n",
    "from matplotlib import pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
    "import re\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import svm\n",
    "def clean(text):\n",
    "    url = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
    "    number = re.compile(r\"[0-9]\")\n",
    "    speacial_f = re.compile(r\"\\x89Û_\")\n",
    "    speacial_s = re.compile(r\"\\x89ÛÒ\")\n",
    "    speacial_t = re.compile(r\"\\x89ÛÏ\")\n",
    "    first_row = url.sub(r\"\", text)\n",
    "    second_row = number.sub(r\"\",first_row)\n",
    "    third_row = speacial_f.sub(r\"\",second_row)\n",
    "    fourth_row = speacial_s.sub(r\"\",third_row)\n",
    "    return speacial_t.sub(r\"\", fourth_row)\n",
    "\n",
    "def remove_punct(text):\n",
    "    translator = str.maketrans(\"\", \"\", string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "def load_data():\n",
    "    train_raw_data = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\n",
    "    test_raw_data = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\n",
    "    return train_raw_data['text'], train_raw_data['target'], test_raw_data\n",
    "\n",
    "def URLandpunct(data):\n",
    "    data = data.map(clean)\n",
    "    data = data.map(remove_punct)\n",
    "    return data\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    filtered_words = [word.lower() for word in text.split() if word.lower() not in stopwords.words('english')]\n",
    "    return \" \".join(filtered_words)\n",
    "if __name__ == '__main__':\n",
    "    train_raw_X, train_raw_Y, expriment = load_data()\n",
    "    exprimentX = expriment['text']     \n",
    "    #eliminating URL and others\n",
    "    # train\n",
    "    trainX = URLandpunct(train_raw_X)\n",
    "    exprimentX = URLandpunct(exprimentX)\n",
    "    #stopwords\n",
    "    trainX = trainX.map(remove_stopwords)\n",
    "    exprimentX = exprimentX.map(remove_stopwords)\n",
    "    #vectorization\n",
    "    tfidf = TfidfVectorizer(sublinear_tf=True, max_features=60000, min_df=1, norm='l2',  ngram_range=(1,2))\n",
    "    features = tfidf.fit_transform(trainX)\n",
    "    features_test = tfidf.transform(exprimentX)\n",
    "    #Getting test set\n",
    "    #split data \n",
    "    skf = StratifiedKFold(n_splits=5, random_state=50, shuffle=True)\n",
    "    accuracy = [] \n",
    "    n = 1\n",
    "    predictions = pd.DataFrame()\n",
    "    for train_idx, test_idx in skf.split(features,train_raw_Y):\n",
    "        X_train,X_val=features[train_idx],features[test_idx]\n",
    "        y_train,y_val=train_raw_Y.iloc[train_idx],train_raw_Y.iloc[test_idx]\n",
    "        #model= LogisticRegression(max_iter=1000,C=3)\n",
    "        model=MultinomialNB(alpha=0.5)\n",
    "        #model=svm.SVC(max_iter=1000)\n",
    "        model.fit(X_train,y_train)\n",
    "        s = model.predict(X_val)\n",
    "        accuracy.append(accuracy_score(y_val, s))\n",
    "        predictions[str(n)] = model.predict(features_test) \n",
    "        n+=1\n",
    "    print(np.array(accuracy).mean())\n",
    "    df=predictions[['1','2','3','4','5']].mode(axis=1)\n",
    "    expriment['target']=df[0]\n",
    "    print(expriment)\n",
    "    expriment.to_csv(\"mysubmission.csv\",index=False)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86764454",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-29T06:40:09.168603Z",
     "iopub.status.busy": "2021-12-29T06:40:09.163213Z",
     "iopub.status.idle": "2021-12-29T06:47:37.235214Z",
     "shell.execute_reply": "2021-12-29T06:47:37.234594Z",
     "shell.execute_reply.started": "2021-12-29T06:30:09.106255Z"
    },
    "papermill": {
     "duration": 448.088052,
     "end_time": "2021-12-29T06:47:37.235395",
     "exception": false,
     "start_time": "2021-12-29T06:40:09.147343",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "User settings:\n",
      "\n",
      "   KMP_AFFINITY=granularity=fine,verbose,compact,1,0\n",
      "   KMP_BLOCKTIME=0\n",
      "   KMP_DUPLICATE_LIB_OK=True\n",
      "   KMP_INIT_AT_FORK=FALSE\n",
      "   KMP_SETTINGS=1\n",
      "   KMP_WARNINGS=0\n",
      "\n",
      "Effective settings:\n",
      "\n",
      "   KMP_ABORT_DELAY=0\n",
      "   KMP_ADAPTIVE_LOCK_PROPS='1,1024'\n",
      "   KMP_ALIGN_ALLOC=64\n",
      "   KMP_ALL_THREADPRIVATE=128\n",
      "   KMP_ATOMIC_MODE=2\n",
      "   KMP_BLOCKTIME=0\n",
      "   KMP_CPUINFO_FILE: value is not defined\n",
      "   KMP_DETERMINISTIC_REDUCTION=false\n",
      "   KMP_DEVICE_THREAD_LIMIT=2147483647\n",
      "   KMP_DISP_NUM_BUFFERS=7\n",
      "   KMP_DUPLICATE_LIB_OK=true\n",
      "   KMP_ENABLE_TASK_THROTTLING=true\n",
      "   KMP_FORCE_REDUCTION: value is not defined\n",
      "   KMP_FOREIGN_THREADS_THREADPRIVATE=true\n",
      "   KMP_FORKJOIN_BARRIER='2,2'\n",
      "   KMP_FORKJOIN_BARRIER_PATTERN='hyper,hyper'\n",
      "   KMP_GTID_MODE=3\n",
      "   KMP_HANDLE_SIGNALS=false\n",
      "   KMP_HOT_TEAMS_MAX_LEVEL=1\n",
      "   KMP_HOT_TEAMS_MODE=0\n",
      "   KMP_INIT_AT_FORK=true\n",
      "   KMP_LIBRARY=throughput\n",
      "   KMP_LOCK_KIND=queuing\n",
      "   KMP_MALLOC_POOL_INCR=1M\n",
      "   KMP_NUM_LOCKS_IN_BLOCK=1\n",
      "   KMP_PLAIN_BARRIER='2,2'\n",
      "   KMP_PLAIN_BARRIER_PATTERN='hyper,hyper'\n",
      "   KMP_REDUCTION_BARRIER='1,1'\n",
      "   KMP_REDUCTION_BARRIER_PATTERN='hyper,hyper'\n",
      "   KMP_SCHEDULE='static,balanced;guided,iterative'\n",
      "   KMP_SETTINGS=true\n",
      "   KMP_SPIN_BACKOFF_PARAMS='4096,100'\n",
      "   KMP_STACKOFFSET=64\n",
      "   KMP_STACKPAD=0\n",
      "   KMP_STACKSIZE=8M\n",
      "   KMP_STORAGE_MAP=false\n",
      "   KMP_TASKING=2\n",
      "   KMP_TASKLOOP_MIN_TASKS=0\n",
      "   KMP_TASK_STEALING_CONSTRAINT=1\n",
      "   KMP_TEAMS_THREAD_LIMIT=4\n",
      "   KMP_TOPOLOGY_METHOD=all\n",
      "   KMP_USE_YIELD=1\n",
      "   KMP_VERSION=false\n",
      "   KMP_WARNINGS=false\n",
      "   OMP_AFFINITY_FORMAT='OMP: pid %P tid %i thread %n bound to OS proc set {%A}'\n",
      "   OMP_ALLOCATOR=omp_default_mem_alloc\n",
      "   OMP_CANCELLATION=false\n",
      "   OMP_DEFAULT_DEVICE=0\n",
      "   OMP_DISPLAY_AFFINITY=false\n",
      "   OMP_DISPLAY_ENV=false\n",
      "   OMP_DYNAMIC=false\n",
      "   OMP_MAX_ACTIVE_LEVELS=1\n",
      "   OMP_MAX_TASK_PRIORITY=0\n",
      "   OMP_NESTED: deprecated; max-active-levels-var=1\n",
      "   OMP_NUM_THREADS: value is not defined\n",
      "   OMP_PLACES: value is not defined\n",
      "   OMP_PROC_BIND='intel'\n",
      "   OMP_SCHEDULE='static'\n",
      "   OMP_STACKSIZE=8M\n",
      "   OMP_TARGET_OFFLOAD=DEFAULT\n",
      "   OMP_THREAD_LIMIT=2147483647\n",
      "   OMP_WAIT_POLICY=PASSIVE\n",
      "   KMP_AFFINITY='verbose,warnings,respect,granularity=fine,compact,1,0'\n",
      "\n",
      "2021-12-29 06:40:37.415746: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "2021-12-29 06:40:37.927253: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "191/191 [==============================] - 24s 111ms/step - loss: 0.7494 - accuracy: 0.7049 - val_loss: 0.5511 - val_accuracy: 0.8043\n",
      "Epoch 2/20\n",
      "191/191 [==============================] - 21s 108ms/step - loss: 0.4381 - accuracy: 0.8635 - val_loss: 0.5167 - val_accuracy: 0.8050\n",
      "Epoch 3/20\n",
      "191/191 [==============================] - 20s 103ms/step - loss: 0.3918 - accuracy: 0.8882 - val_loss: 0.5385 - val_accuracy: 0.7984\n",
      "Epoch 4/20\n",
      "191/191 [==============================] - 21s 109ms/step - loss: 0.3242 - accuracy: 0.9251 - val_loss: 0.5504 - val_accuracy: 0.7951\n",
      "Epoch 5/20\n",
      "191/191 [==============================] - 20s 103ms/step - loss: 0.2838 - accuracy: 0.9433 - val_loss: 0.5742 - val_accuracy: 0.7873\n",
      "Epoch 6/20\n",
      "191/191 [==============================] - 20s 106ms/step - loss: 0.2637 - accuracy: 0.9521 - val_loss: 0.5980 - val_accuracy: 0.7859\n",
      "Epoch 7/20\n",
      "191/191 [==============================] - 21s 109ms/step - loss: 0.2469 - accuracy: 0.9583 - val_loss: 0.5925 - val_accuracy: 0.7859\n",
      "Epoch 8/20\n",
      "191/191 [==============================] - 20s 103ms/step - loss: 0.2366 - accuracy: 0.9616 - val_loss: 0.6230 - val_accuracy: 0.7689\n",
      "Epoch 9/20\n",
      "191/191 [==============================] - 20s 107ms/step - loss: 0.2265 - accuracy: 0.9635 - val_loss: 0.6399 - val_accuracy: 0.7735\n",
      "Epoch 10/20\n",
      "191/191 [==============================] - 20s 107ms/step - loss: 0.2140 - accuracy: 0.9698 - val_loss: 0.6501 - val_accuracy: 0.7702\n",
      "Epoch 11/20\n",
      "191/191 [==============================] - 20s 107ms/step - loss: 0.2108 - accuracy: 0.9688 - val_loss: 0.6612 - val_accuracy: 0.7571\n",
      "Epoch 12/20\n",
      "191/191 [==============================] - 20s 106ms/step - loss: 0.2046 - accuracy: 0.9711 - val_loss: 0.6724 - val_accuracy: 0.7708\n",
      "Epoch 13/20\n",
      "191/191 [==============================] - 20s 107ms/step - loss: 0.1987 - accuracy: 0.9726 - val_loss: 0.6678 - val_accuracy: 0.7610\n",
      "Epoch 14/20\n",
      "191/191 [==============================] - 21s 111ms/step - loss: 0.1930 - accuracy: 0.9742 - val_loss: 0.6839 - val_accuracy: 0.7577\n",
      "Epoch 15/20\n",
      "191/191 [==============================] - 21s 108ms/step - loss: 0.1902 - accuracy: 0.9749 - val_loss: 0.6803 - val_accuracy: 0.7590\n",
      "Epoch 16/20\n",
      "191/191 [==============================] - 20s 107ms/step - loss: 0.1874 - accuracy: 0.9742 - val_loss: 0.6891 - val_accuracy: 0.7603\n",
      "Epoch 17/20\n",
      "191/191 [==============================] - 21s 108ms/step - loss: 0.1826 - accuracy: 0.9765 - val_loss: 0.7017 - val_accuracy: 0.7590\n",
      "Epoch 18/20\n",
      "191/191 [==============================] - 20s 104ms/step - loss: 0.1810 - accuracy: 0.9759 - val_loss: 0.6923 - val_accuracy: 0.7636\n",
      "Epoch 19/20\n",
      "191/191 [==============================] - 21s 109ms/step - loss: 0.1786 - accuracy: 0.9767 - val_loss: 0.7214 - val_accuracy: 0.7571\n",
      "Epoch 20/20\n",
      "191/191 [==============================] - 21s 108ms/step - loss: 0.1761 - accuracy: 0.9760 - val_loss: 0.7149 - val_accuracy: 0.7544\n",
      "0.7544320225715637\n",
      "Complete\n",
      "[[0.93620086]\n",
      " [0.9382874 ]\n",
      " [0.9387245 ]\n",
      " ...\n",
      " [0.9292027 ]\n",
      " [0.93978024]\n",
      " [0.94000876]]\n"
     ]
    }
   ],
   "source": [
    "import keras.models\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "from keras.applications.densenet import layers\n",
    "from keras.regularizers import l2\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras_preprocessing.text import Tokenizer\n",
    "from matplotlib import pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "# total words\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "max_words = 10000\n",
    "# text lenth\n",
    "maxlen = 100\n",
    "#output dimension\n",
    "embedding_dim = 128\n",
    "\n",
    "def clean(text):\n",
    "    url = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
    "    number = re.compile(r\"[0-9]\")\n",
    "    speacial_f = re.compile(r\"\\x89Û_\")\n",
    "    speacial_s = re.compile(r\"\\x89ÛÒ\")\n",
    "    speacial_t = re.compile(r\"\\x89ÛÏ\")\n",
    "    first_row = url.sub(r\"\", text)\n",
    "    second_row = number.sub(r\"\",first_row)\n",
    "    third_row = speacial_f.sub(r\"\",second_row)\n",
    "    fourth_row = speacial_s.sub(r\"\",third_row)\n",
    "    return speacial_t.sub(r\"\", fourth_row)\n",
    "\n",
    "def remove_punct(text):\n",
    "    translator = str.maketrans(\"\", \"\", string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "def load_data():\n",
    "    train_raw_data = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\n",
    "    test_raw_data = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\n",
    "    return train_raw_data['text'], train_raw_data['target'], test_raw_data\n",
    "\n",
    "def URLandpunct(data):\n",
    "    data = data.map(clean)\n",
    "    data = data.map(remove_punct)\n",
    "    return data\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    filtered_words = [word.lower() for word in text.split() if word.lower() not in stopwords.words('english')]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "\n",
    "def tokenized(dataframe):\n",
    "    # 1、Processing\n",
    "    tokenizer = Tokenizer(num_words=max_words)\n",
    "    tokenizer.fit_on_texts(dataframe)\n",
    "    vocab = len(tokenizer.word_index)+1\n",
    "    train_X = tokenizer.texts_to_sequences(dataframe)\n",
    "    # 2、padding\n",
    "    train_X = pad_sequences(train_X, maxlen=maxlen)\n",
    "    return train_X, vocab\n",
    "if __name__ == '__main__':\n",
    "    print(\"Loading\")\n",
    "    trainX, trainY, expriment = load_data()\n",
    "    exprimentX = expriment['text']\n",
    "    #eliminating URL and others\n",
    "    # train\n",
    "    trainX = URLandpunct(trainX)\n",
    "    exprimentX = URLandpunct(exprimentX)\n",
    "    #stopwords\n",
    "    trainX = trainX.map(remove_stopwords)\n",
    "    exprimentX = exprimentX.map(remove_stopwords)\n",
    "    #Tokenizing\n",
    "    trainX, train_words = tokenized(trainX)\n",
    "    exprimentX, train_words = tokenized(exprimentX)\n",
    "    #Getting test set\n",
    "    trainX, testX, trainY, testY = train_test_split(trainX, trainY, test_size=0.2)\n",
    "    #Modelling\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(layers.Embedding(max_words, embedding_dim, input_length=maxlen))\n",
    "    model.add(layers.LSTM(32, dropout=0.20))\n",
    "    model.add(layers.Dense(1, activation=\"sigmoid\", kernel_regularizer=l2(0.2)))\n",
    "    # model.summary()\n",
    "    model.compile(optimizer=\"adam\",\n",
    "                  loss=\"BinaryCrossentropy\",\n",
    "                  metrics=['accuracy'],\n",
    "    )\n",
    "    history = model.fit(\n",
    "        trainX,\n",
    "        trainY,\n",
    "        epochs=20,\n",
    "        validation_data=(testX, testY)\n",
    "    )\n",
    "    score = model.evaluate(testX, testY, verbose=0)\n",
    "    exprimentY = model.predict(exprimentX)\n",
    "    predictions = [1 if p > 0.5 else 0 for p in exprimentY]\n",
    "    print(score[1])\n",
    "    print(\"Complete\")\n",
    "    print(exprimentY)\n",
    "    submission = pd.DataFrame({'id': expriment['id'].values.tolist(), 'target': predictions})\n",
    "    submission.to_csv('submission.csv', index=False)\n",
    "    #visulization\n",
    "    #accuracy\n",
    "    # acc = history.history['accuracy']\n",
    "    # val_acc = history.history['val_accuracy']\n",
    "    # loss = history.history['loss']\n",
    "    # val_loss = history.history['val_loss']\n",
    "    # plt.subplot(1, 2, 1)\n",
    "    # plt.plot(acc, label='Training Accuracy')\n",
    "    # plt.plot(val_acc, label='Validation Accuracy')\n",
    "    # plt.title('Training and Validation Accuracy')\n",
    "    # plt.legend()\n",
    "    # #loss\n",
    "    # loss = history.history['loss']\n",
    "    # val_loss = history.history['val_loss']\n",
    "    # plt.subplot(1, 2, 2)\n",
    "    # plt.plot(loss, label='Training Loss')\n",
    "    # plt.plot(val_loss, label='Validation Loss')\n",
    "    # plt.title('Training and Validation Loss')\n",
    "    # plt.legend()\n",
    "    # plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 502.271584,
   "end_time": "2021-12-29T06:47:41.576480",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-12-29T06:39:19.304896",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
